"""
Test DQN Agent - Comprehensive Testing Suite

This script tests the DQN agent implementation including:
1. Agent creation with different configurations
2. Action selection (exploration vs exploitation)
3. Experience observation and replay buffer integration
4. Training loop and Q-learning updates
5. Target network updates
6. Epsilon decay
7. Save/load functionality
8. Integration with Connect 4 environment

Run this script to verify the DQN agent works correctly before training.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

import numpy as np
import torch
from environment import ConnectFourEnvironment, Config
from agents import DQNAgent, RandomAgent


def print_section(title):
    """Print a formatted section header."""
    print("\n" + "=" * 60)
    print(title)
    print("=" * 60 + "\n")


def test_agent_creation():
    """Test creating DQN agents with different configurations."""
    print_section("Test 1: Agent Creation")
    
    # Default agent
    print("âœ“ Creating default DQN agent...")
    agent1 = DQNAgent(name="DQN-Default", player_id=1)
    print(f"  {agent1}")
    print(f"  Device: {agent1.device}")
    print(f"  Q-network parameters: {sum(p.numel() for p in agent1.q_network.parameters()):,}")
    print(f"  Epsilon: {agent1.epsilon}")
    print(f"  Gamma: {agent1.gamma}")
    print(f"  Batch size: {agent1.batch_size}")
    
    # Deep network
    print("\nâœ“ Creating deep DQN agent...")
    agent2 = DQNAgent(
        name="DQN-Deep",
        player_id=-1,
        conv_channels=[32, 64, 128],
        fc_dims=[256, 128],
        dropout_rate=0.2,
        learning_rate=5e-4
    )
    print(f"  {agent2}")
    print(f"  Q-network parameters: {sum(p.numel() for p in agent2.q_network.parameters()):,}")
    
    # Shallow network
    print("\nâœ“ Creating shallow DQN agent...")
    agent3 = DQNAgent(
        name="DQN-Shallow",
        conv_channels=[32],
        fc_dims=[64],
        epsilon_start=0.5
    )
    print(f"  {agent3}")
    print(f"  Q-network parameters: {sum(p.numel() for p in agent3.q_network.parameters()):,}")
    
    print("\nâœ… Agent creation test passed!")
    return agent1


def test_action_selection(agent):
    """Test action selection with exploration vs exploitation."""
    print_section("Test 2: Action Selection")
    
    config = Config()
    env = ConnectFourEnvironment(config)
    state = env.get_state()
    legal_moves = env.get_legal_moves()
    
    # Test exploration (high epsilon)
    print("âœ“ Testing exploration (Îµ=1.0)...")
    agent.set_exploration(1.0)
    actions = [agent.select_action(state, legal_moves) for _ in range(10)]
    print(f"  Actions with Îµ=1.0: {actions}")
    print(f"  Unique actions: {len(set(actions))}/10 (should be diverse)")
    
    # Test exploitation (low epsilon)
    print("\nâœ“ Testing exploitation (Îµ=0.0)...")
    agent.set_exploration(0.0)
    actions = [agent.select_action(state, legal_moves) for _ in range(10)]
    print(f"  Actions with Îµ=0.0: {actions}")
    print(f"  Unique actions: {len(set(actions))}/10 (should be same)")
    
    # Test with restricted legal moves
    print("\nâœ“ Testing with restricted legal moves...")
    restricted_moves = [0, 3, 6]
    actions = [agent.select_action(state, restricted_moves) for _ in range(10)]
    print(f"  Legal moves: {restricted_moves}")
    print(f"  Selected actions: {actions}")
    print(f"  All legal: {all(a in restricted_moves for a in actions)}")
    
    print("\nâœ… Action selection test passed!")


def test_observation_and_buffer(agent):
    """Test experience observation and replay buffer."""
    print_section("Test 3: Observation and Replay Buffer")
    
    config = Config()
    env = ConnectFourEnvironment(config)
    
    print("âœ“ Playing a few moves and observing...")
    for i in range(5):
        state = env.get_state()
        legal_moves = env.get_legal_moves()
        action = agent.select_action(state, legal_moves)
        next_state, reward, done = env.play_move(action)
        agent.observe(state, action, reward, next_state, done)
        print(f"  Step {i+1}: action={action}, reward={reward}, done={done}")
    
    print(f"\nâœ“ Replay buffer stats:")
    stats = agent.replay_buffer.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    print("\nâœ… Observation and buffer test passed!")


def test_training(agent):
    """Test training loop."""
    print_section("Test 4: Training")
    
    config = Config()
    env = ConnectFourEnvironment(config)
    
    # Collect enough experiences
    print("âœ“ Collecting experiences for training...")
    agent.set_exploration(1.0)  # Random exploration
    
    for episode in range(20):
        env.reset()
        done = False
        steps = 0
        
        while not done and steps < 42:
            state = env.get_state()
            legal_moves = env.get_legal_moves()
            action = agent.select_action(state, legal_moves)
            next_state, reward, done = env.play_move(action)
            agent.observe(state, action, reward, next_state, done)
            steps += 1
    
    print(f"  Buffer size: {len(agent.replay_buffer)}")
    
    # Train multiple times
    print("\nâœ“ Training agent...")
    losses = []
    for i in range(10):
        metrics = agent.train()
        if metrics:
            losses.append(metrics['loss'])
            if i == 0 or i == 9:
                print(f"  Step {i+1}:")
                print(f"    Loss: {metrics['loss']:.4f}")
                print(f"    Mean Q-value: {metrics['mean_q_value']:.4f}")
                print(f"    Train steps: {metrics['train_steps']}")
    
    print(f"\nâœ“ Training statistics:")
    print(f"  Total training steps: {agent.train_steps}")
    print(f"  Average loss: {np.mean(losses):.4f}")
    print(f"  Loss std: {np.std(losses):.4f}")
    
    print("\nâœ… Training test passed!")


def test_target_network_update(agent):
    """Test target network updates."""
    print_section("Test 5: Target Network Update")
    
    # Get initial target network parameters
    initial_target_params = [p.clone() for p in agent.target_network.parameters()]
    
    print("âœ“ Training to trigger target network update...")
    print(f"  Initial train steps: {agent.train_steps}")
    print(f"  Target update frequency: {agent.target_update_freq}")
    
    # Train until target network should update
    steps_needed = agent.target_update_freq - (agent.train_steps % agent.target_update_freq)
    print(f"  Steps needed for update: {steps_needed}")
    
    for _ in range(steps_needed + 1):
        metrics = agent.train()
        if metrics:
            pass  # Just training
    
    print(f"  Final train steps: {agent.train_steps}")
    
    # Check if target network was updated
    final_target_params = [p.clone() for p in agent.target_network.parameters()]
    
    # Compare Q-network and target network
    q_params = [p.clone() for p in agent.q_network.parameters()]
    networks_match = all(
        torch.allclose(q_p, t_p) 
        for q_p, t_p in zip(q_params, final_target_params)
    )
    
    print(f"\nâœ“ Q-network and target network match: {networks_match}")
    print("  (They should match after target update)")
    
    print("\nâœ… Target network update test passed!")


def test_epsilon_decay(agent):
    """Test epsilon decay."""
    print_section("Test 6: Epsilon Decay")
    
    print("âœ“ Testing epsilon decay over episodes...")
    print(f"  Initial epsilon: {agent.epsilon:.4f}")
    print(f"  Epsilon decay rate: {agent.epsilon_decay}")
    print(f"  Epsilon end: {agent.epsilon_end}")
    
    epsilons = [agent.epsilon]
    for episode in range(100):
        agent.decay_epsilon()
        if episode % 20 == 19:
            epsilons.append(agent.epsilon)
            print(f"  Episode {episode+1}: Îµ={agent.epsilon:.4f}")
    
    print(f"\nâœ“ Final epsilon: {agent.epsilon:.4f}")
    print(f"  Reached minimum: {agent.epsilon <= agent.epsilon_end}")
    
    print("\nâœ… Epsilon decay test passed!")


def test_save_load():
    """Test save and load functionality."""
    print_section("Test 7: Save/Load")
    
    # Create and train agent
    print("âœ“ Creating and training agent...")
    agent1 = DQNAgent(name="DQN-Save-Test", epsilon_start=0.5)
    
    config = Config()
    env = ConnectFourEnvironment(config)
    for _ in range(10):
        env.reset()
        state = env.get_state()
        action = agent1.select_action(state, env.get_legal_moves())
        next_state, reward, done = env.play_move(action)
        agent1.observe(state, action, reward, next_state, done)
    
    for _ in range(5):
        agent1.train()
    
    agent1.decay_epsilon()
    agent1.wins = 5
    agent1.losses = 3
    agent1.draws = 2
    
    print(f"  Original agent: {agent1}")
    print(f"  Train steps: {agent1.train_steps}")
    print(f"  Epsilon: {agent1.epsilon:.4f}")
    
    # Save agent
    save_path = "/tmp/test_dqn_agent.pth"
    print(f"\nâœ“ Saving agent to {save_path}...")
    agent1.save(save_path)
    
    # Create new agent and load
    print("\nâœ“ Creating new agent and loading...")
    agent2 = DQNAgent(name="DQN-Load-Test")
    agent2.load(save_path)
    
    print(f"  Loaded agent: {agent2}")
    print(f"  Train steps: {agent2.train_steps}")
    print(f"  Epsilon: {agent2.epsilon:.4f}")
    
    # Verify stats match
    print("\nâœ“ Verifying loaded stats match original...")
    print(f"  Train steps match: {agent1.train_steps == agent2.train_steps}")
    print(f"  Epsilon match: {abs(agent1.epsilon - agent2.epsilon) < 1e-6}")
    print(f"  Wins match: {agent1.wins == agent2.wins}")
    print(f"  Losses match: {agent1.losses == agent2.losses}")
    
    # Clean up
    import os
    os.remove(save_path)
    
    print("\nâœ… Save/load test passed!")


def test_game_integration():
    """Test DQN agent playing against random agent."""
    print_section("Test 8: Game Integration")
    
    print("âœ“ Creating agents...")
    dqn_agent = DQNAgent(name="DQN-Player", player_id=1, epsilon_start=0.3)
    random_agent = RandomAgent(name="Random-Player", player_id=-1)
    
    print("âœ“ Playing a complete game...")
    config = Config()
    env = ConnectFourEnvironment(config)
    env.reset()
    
    current_player = 1
    move_count = 0
    
    while True:
        state = env.get_state()
        legal_moves = env.get_legal_moves()
        
        # Select agent
        if current_player == 1:
            action = dqn_agent.select_action(state, legal_moves)
            agent_name = "DQN"
        else:
            action = random_agent.select_action(state, legal_moves)
            agent_name = "Random"
        
        # Play move
        next_state, reward, done = env.play_move(action)
        move_count += 1
        
        print(f"  Move {move_count}: {agent_name} plays column {action}")
        
        # Store experience for DQN agent
        if current_player == 1:
            dqn_agent.observe(state, action, reward, next_state, done)
        
        if done:
            winner = env.check_winner()
            if winner == 1:
                print(f"\nâœ“ DQN agent wins!")
                dqn_agent.update_game_result('win')
                random_agent.update_game_result('loss')
            elif winner == -1:
                print(f"\nâœ“ Random agent wins!")
                dqn_agent.update_game_result('loss')
                random_agent.update_game_result('win')
            else:
                print(f"\nâœ“ Draw!")
                dqn_agent.update_game_result('draw')
                random_agent.update_game_result('draw')
            break
        
        current_player *= -1
    
    print(f"\nâœ“ Game statistics:")
    print(f"  DQN: {dqn_agent.get_stats()}")
    print(f"  Random: {random_agent.get_stats()}")
    print(f"  DQN buffer size: {len(dqn_agent.replay_buffer)}")
    
    print("\nâœ… Game integration test passed!")


def main():
    """Run all tests."""
    print("=" * 60)
    print("DQN AGENT TEST SUITE")
    print("=" * 60)
    
    # Run tests
    agent = test_agent_creation()
    test_action_selection(agent)
    test_observation_and_buffer(agent)
    test_training(agent)
    test_target_network_update(agent)
    test_epsilon_decay(agent)
    test_save_load()
    test_game_integration()
    
    # Final summary
    print_section("ðŸŽ‰ ALL TESTS PASSED!")
    print("The DQN Agent is working correctly!")
    print("\nFeatures verified:")
    print("  âœ“ Agent creation with configurable architecture")
    print("  âœ“ Îµ-greedy action selection")
    print("  âœ“ Experience replay buffer integration")
    print("  âœ“ Q-learning training loop")
    print("  âœ“ Target network updates")
    print("  âœ“ Epsilon decay")
    print("  âœ“ Save/load functionality")
    print("  âœ“ Full game integration")
    print("\nReady to train DQN agent!")
    print("=" * 60)


if __name__ == "__main__":
    main()