{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DQN Training Audit - Verify Q-Value Updates\n",
                "\n",
                "This notebook tests the DQN agent's training mechanism to verify:\n",
                "1. Q-values increase for winning moves after training\n",
                "2. Training propagates backward to previous states (credit assignment)\n",
                "3. The Bellman equation is implemented correctly\n",
                "\n",
                "## Test Scenario\n",
                "- Create a win-in-1 position\n",
                "- Train on the winning move\n",
                "- Verify Q-value increases toward target (reward = 1.0)\n",
                "- Train on the previous move that led to win-in-1\n",
                "- Verify Q-value increases (should learn to get into winning positions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ImportError",
                    "evalue": "cannot import name 'Connect4Env' from 'src.environment.connect4' (/Users/cmcol/Development/github/connect4-rl/notebooks/../src/environment/connect4.py)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Add project root to path\u001b[39;00m\n\u001b[32m      6\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvironment\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnect4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Connect4Env\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdqn_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DQNAgent\n",
                        "\u001b[31mImportError\u001b[39m: cannot import name 'Connect4Env' from 'src.environment.connect4' (/Users/cmcol/Development/github/connect4-rl/notebooks/../src/environment/connect4.py)"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Add project root to path\n",
                "sys.path.append('..')\n",
                "\n",
                "from src.environment.connect4 import Connect4Env\n",
                "from src.agents.dqn_agent import DQNAgent"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment and Agent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create environment\n",
                "env = Connect4Env()\n",
                "\n",
                "# Create DQN agent with small network for faster testing\n",
                "agent = DQNAgent(\n",
                "    name=\"DQN-Audit\",\n",
                "    player_id=1,\n",
                "    conv_channels=[16, 32],  # Smaller network\n",
                "    fc_dims=[64],\n",
                "    learning_rate=1e-3,  # Higher LR for faster learning in test\n",
                "    gamma=0.99,\n",
                "    batch_size=1,  # Train on single experience\n",
                "    min_buffer_size=1,  # Allow training immediately\n",
                "    buffer_size=1000,\n",
                "    target_update_freq=1000,\n",
                "    use_double_dqn=True\n",
                ")\n",
                "\n",
                "print(f\"Agent created: {agent}\")\n",
                "print(f\"Device: {agent.device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create Win-in-1 Test Scenario\n",
                "\n",
                "We'll create a position where Player 1 can win by playing column 3."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_win_in_1_scenario():\n",
                "    \"\"\"\n",
                "    Create a win-in-1 position for Player 1.\n",
                "    \n",
                "    Board layout (Player 1 = X, Player -1 = O):\n",
                "    . . . . . . .\n",
                "    . . . . . . .\n",
                "    . . . . . . .\n",
                "    . . . . . . .\n",
                "    . . . . . . .\n",
                "    X X X . O O .  <- Player 1 can win at column 3\n",
                "    0 1 2 3 4 5 6\n",
                "    \"\"\"\n",
                "    env.reset()\n",
                "    \n",
                "    # Player 1 plays columns 0, 1, 2\n",
                "    env.play_move(0)  # Player 1\n",
                "    env.play_move(4)  # Player -1\n",
                "    env.play_move(1)  # Player 1\n",
                "    env.play_move(5)  # Player -1\n",
                "    env.play_move(2)  # Player 1\n",
                "    \n",
                "    # Now it's Player -1's turn, but we'll switch to Player 1 for testing\n",
                "    env.current_player = 1\n",
                "    \n",
                "    return env\n",
                "\n",
                "# Create scenario\n",
                "env = create_win_in_1_scenario()\n",
                "state_before_win = env.get_state()\n",
                "\n",
                "# Visualize\n",
                "print(\"Win-in-1 Position (Player 1's turn):\")\n",
                "env.render()\n",
                "print(f\"\\nPlayer 1 can win by playing column 3\")\n",
                "print(f\"Legal moves: {env.get_legal_moves()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Test 1: Train on Winning Move\n",
                "\n",
                "Train the agent on the winning move and verify Q-value increases."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get state before winning move\n",
                "state_before_win = env.get_state()\n",
                "\n",
                "# Make winning move\n",
                "winning_action = 3\n",
                "next_state, reward, done, info = env.step(winning_action)\n",
                "\n",
                "print(\"After winning move:\")\n",
                "env.render()\n",
                "print(f\"\\nReward: {reward}\")\n",
                "print(f\"Done: {done}\")\n",
                "print(f\"Winner: {info.get('winner')}\")\n",
                "\n",
                "# Audit the training step\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"AUDITING TRAINING ON WINNING MOVE\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "audit_results = agent.audit_training_step(\n",
                "    state=state_before_win,\n",
                "    action=winning_action,\n",
                "    reward=reward,\n",
                "    next_state=next_state,\n",
                "    done=done\n",
                ")\n",
                "\n",
                "# Display results\n",
                "print(f\"\\nAction taken: Column {winning_action}\")\n",
                "print(f\"Reward: {audit_results['reward']}\")\n",
                "print(f\"Done: {audit_results['done']}\")\n",
                "print(f\"\\nQ-values for all actions BEFORE training:\")\n",
                "for col in range(7):\n",
                "    marker = \" <-- WINNING MOVE\" if col == winning_action else \"\"\n",
                "    print(f\"  Column {col}: {audit_results['q_before'][col]:+.4f}{marker}\")\n",
                "\n",
                "print(f\"\\nTraining metrics:\")\n",
                "print(f\"  Target Q-value: {audit_results['target_q']:+.4f}\")\n",
                "print(f\"  Q-value before: {audit_results['q_action_before']:+.4f}\")\n",
                "print(f\"  Q-value after:  {audit_results['q_action_after']:+.4f}\")\n",
                "print(f\"  Change:         {audit_results['q_change']:+.4f}\")\n",
                "print(f\"  TD Error:       {audit_results['td_error']:+.4f}\")\n",
                "print(f\"  Loss:           {audit_results['loss']:.6f}\")\n",
                "\n",
                "print(f\"\\nQ-values for all actions AFTER training:\")\n",
                "for col in range(7):\n",
                "    marker = \" <-- WINNING MOVE\" if col == winning_action else \"\"\n",
                "    print(f\"  Column {col}: {audit_results['q_after'][col]:+.4f}{marker}\")\n",
                "\n",
                "# Verify learning\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"VERIFICATION\")\n",
                "print(\"=\"*60)\n",
                "if audit_results['q_change'] > 0:\n",
                "    print(\"✓ SUCCESS: Q-value increased toward target (reward = 1.0)\")\n",
                "    print(f\"  Q-value moved from {audit_results['q_action_before']:.4f} toward {audit_results['target_q']:.4f}\")\n",
                "else:\n",
                "    print(\"✗ FAILURE: Q-value did not increase\")\n",
                "    print(f\"  This indicates a problem with the training implementation\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Test 2: Credit Assignment - Train on Previous Move\n",
                "\n",
                "Now test if training propagates backward. Train on the move that led to the win-in-1 position."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the position BEFORE the win-in-1 (one move earlier)\n",
                "def create_pre_win_scenario():\n",
                "    \"\"\"\n",
                "    Create position one move before win-in-1.\n",
                "    \n",
                "    Board layout:\n",
                "    . . . . . . .\n",
                "    . . . . . . .\n",
                "    . . . . . . .\n",
                "    . . . . . . .\n",
                "    . . . . . . .\n",
                "    X X . . O O .  <- Player 1 plays column 2 to create win-in-1\n",
                "    0 1 2 3 4 5 6\n",
                "    \"\"\"\n",
                "    env.reset()\n",
                "    \n",
                "    # Player 1 plays columns 0, 1\n",
                "    env.play_move(0)  # Player 1\n",
                "    env.play_move(4)  # Player -1\n",
                "    env.play_move(1)  # Player 1\n",
                "    env.play_move(5)  # Player -1\n",
                "    \n",
                "    # Now it's Player 1's turn to play column 2\n",
                "    return env\n",
                "\n",
                "# Create scenario\n",
                "env = create_pre_win_scenario()\n",
                "state_pre_win = env.get_state()\n",
                "\n",
                "print(\"Position BEFORE win-in-1 (Player 1's turn):\")\n",
                "env.render()\n",
                "print(f\"\\nPlayer 1 should play column 2 to create win-in-1 threat\")\n",
                "\n",
                "# Make the move that creates win-in-1\n",
                "setup_action = 2\n",
                "state_after_setup, _, done_setup, _ = env.step(setup_action)\n",
                "\n",
                "print(\"\\nAfter playing column 2:\")\n",
                "env.render()\n",
                "print(f\"\\nNow Player 1 has win-in-1 at column 3 (on next turn)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get Q-values for the setup move BEFORE training\n",
                "q_before_credit = agent.get_q_values(state_pre_win)\n",
                "\n",
                "print(\"Q-values BEFORE training on setup move:\")\n",
                "for col in range(7):\n",
                "    marker = \" <-- SETUP MOVE\" if col == setup_action else \"\"\n",
                "    print(f\"  Column {col}: {q_before_credit[col]:+.4f}{marker}\")\n",
                "\n",
                "# Now audit training on this setup move\n",
                "# The reward is 0 (game continues), but the next state has high Q-value (win-in-1)\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"AUDITING TRAINING ON SETUP MOVE (CREDIT ASSIGNMENT)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "audit_results_2 = agent.audit_training_step(\n",
                "    state=state_pre_win,\n",
                "    action=setup_action,\n",
                "    reward=0.0,  # Game continues\n",
                "    next_state=state_after_setup,\n",
                "    done=False\n",
                ")\n",
                "\n",
                "print(f\"\\nAction taken: Column {setup_action}\")\n",
                "print(f\"Reward: {audit_results_2['reward']} (game continues)\")\n",
                "print(f\"Done: {audit_results_2['done']}\")\n",
                "\n",
                "print(f\"\\nTraining metrics:\")\n",
                "print(f\"  Target Q-value: {audit_results_2['target_q']:+.4f}\")\n",
                "print(f\"  Q-value before: {audit_results_2['q_action_before']:+.4f}\")\n",
                "print(f\"  Q-value after:  {audit_results_2['q_action_after']:+.4f}\")\n",
                "print(f\"  Change:         {audit_results_2['q_change']:+.4f}\")\n",
                "print(f\"  TD Error:       {audit_results_2['td_error']:+.4f}\")\n",
                "print(f\"  Loss:           {audit_results_2['loss']:.6f}\")\n",
                "\n",
                "print(f\"\\nQ-values for all actions AFTER training:\")\n",
                "for col in range(7):\n",
                "    marker = \" <-- SETUP MOVE\" if col == setup_action else \"\"\n",
                "    print(f\"  Column {col}: {audit_results_2['q_after'][col]:+.4f}{marker}\")\n",
                "\n",
                "# Verify credit assignment\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"CREDIT ASSIGNMENT VERIFICATION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# The target should be: reward + gamma * max_Q(next_state)\n",
                "# Since we trained on the winning move, the next state should have high Q-value\n",
                "next_state_q_values = agent.get_q_values(state_after_setup)\n",
                "max_next_q = np.max(next_state_q_values)\n",
                "\n",
                "print(f\"\\nBellman equation breakdown:\")\n",
                "print(f\"  Reward: {audit_results_2['reward']:.4f}\")\n",
                "print(f\"  Gamma: {agent.gamma:.4f}\")\n",
                "print(f\"  Max Q(next_state): {max_next_q:+.4f}\")\n",
                "print(f\"  Expected target: {audit_results_2['reward']} + {agent.gamma} * {max_next_q:.4f} = {audit_results_2['reward'] + agent.gamma * max_next_q:.4f}\")\n",
                "print(f\"  Actual target:   {audit_results_2['target_q']:+.4f}\")\n",
                "\n",
                "if audit_results_2['q_change'] > 0:\n",
                "    print(\"\\n✓ SUCCESS: Credit assignment working!\")\n",
                "    print(f\"  Q-value for setup move increased from {audit_results_2['q_action_before']:.4f} to {audit_results_2['q_action_after']:.4f}\")\n",
                "    print(f\"  This means the agent is learning to value moves that lead to winning positions\")\n",
                "else:\n",
                "    print(\"\\n✗ FAILURE: Credit assignment not working\")\n",
                "    print(f\"  Q-value did not increase for the setup move\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualize Q-Value Changes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create visualization\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Winning move Q-values\n",
                "ax1 = axes[0]\n",
                "columns = np.arange(7)\n",
                "width = 0.35\n",
                "\n",
                "ax1.bar(columns - width/2, audit_results['q_before'], width, label='Before Training', alpha=0.8)\n",
                "ax1.bar(columns + width/2, audit_results['q_after'], width, label='After Training', alpha=0.8)\n",
                "ax1.axhline(y=audit_results['target_q'], color='r', linestyle='--', label=f'Target ({audit_results[\"target_q\"]:.3f})')\n",
                "\n",
                "ax1.set_xlabel('Column')\n",
                "ax1.set_ylabel('Q-Value')\n",
                "ax1.set_title('Test 1: Training on Winning Move (Column 3)')\n",
                "ax1.set_xticks(columns)\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Highlight winning column\n",
                "ax1.axvspan(winning_action - 0.5, winning_action + 0.5, alpha=0.2, color='green')\n",
                "\n",
                "# Plot 2: Setup move Q-values\n",
                "ax2 = axes[1]\n",
                "ax2.bar(columns - width/2, audit_results_2['q_before'], width, label='Before Training', alpha=0.8)\n",
                "ax2.bar(columns + width/2, audit_results_2['q_after'], width, label='After Training', alpha=0.8)\n",
                "ax2.axhline(y=audit_results_2['target_q'], color='r', linestyle='--', label=f'Target ({audit_results_2[\"target_q\"]:.3f})')\n",
                "\n",
                "ax2.set_xlabel('Column')\n",
                "ax2.set_ylabel('Q-Value')\n",
                "ax2.set_title('Test 2: Training on Setup Move (Column 2)')\n",
                "ax2.set_xticks(columns)\n",
                "ax2.legend()\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "# Highlight setup column\n",
                "ax2.axvspan(setup_action - 0.5, setup_action + 0.5, alpha=0.2, color='blue')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nVisualization shows:\")\n",
                "print(\"  - Green highlight: Winning move (should increase toward +1.0)\")\n",
                "print(\"  - Blue highlight: Setup move (should increase based on next state's value)\")\n",
                "print(\"  - Red dashed line: Target Q-value from Bellman equation\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Summary and Conclusions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"AUDIT SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "print(\"\\n1. WINNING MOVE TEST:\")\n",
                "print(f\"   - Q-value change: {audit_results['q_change']:+.4f}\")\n",
                "print(f\"   - Target Q-value: {audit_results['target_q']:+.4f}\")\n",
                "print(f\"   - TD Error: {audit_results['td_error']:+.4f}\")\n",
                "if audit_results['q_change'] > 0:\n",
                "    print(\"   ✓ Q-value increased toward reward (+1.0)\")\n",
                "else:\n",
                "    print(\"   ✗ Q-value did not increase\")\n",
                "\n",
                "print(\"\\n2. CREDIT ASSIGNMENT TEST:\")\n",
                "print(f\"   - Q-value change: {audit_results_2['q_change']:+.4f}\")\n",
                "print(f\"   - Target Q-value: {audit_results_2['target_q']:+.4f}\")\n",
                "print(f\"   - TD Error: {audit_results_2['td_error']:+.4f}\")\n",
                "if audit_results_2['q_change'] > 0:\n",
                "    print(\"   ✓ Q-value increased for move leading to winning position\")\n",
                "    print(\"   ✓ Credit assignment is working\")\n",
                "else:\n",
                "    print(\"   ✗ Q-value did not increase\")\n",
                "    print(\"   ✗ Credit assignment may not be working\")\n",
                "\n",
                "print(\"\\n3. BELLMAN EQUATION VERIFICATION:\")\n",
                "expected_target = audit_results_2['reward'] + agent.gamma * max_next_q\n",
                "target_error = abs(audit_results_2['target_q'] - expected_target)\n",
                "print(f\"   - Expected target: {expected_target:.4f}\")\n",
                "print(f\"   - Actual target:   {audit_results_2['target_q']:.4f}\")\n",
                "print(f\"   - Error:           {target_error:.6f}\")\n",
                "if target_error < 0.01:\n",
                "    print(\"   ✓ Bellman equation implemented correctly\")\n",
                "else:\n",
                "    print(\"   ✗ Bellman equation may have issues\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "if audit_results['q_change'] > 0 and audit_results_2['q_change'] > 0:\n",
                "    print(\"OVERALL: ✓ DQN training is working correctly!\")\n",
                "    print(\"The agent learns to value winning moves and moves that lead to wins.\")\n",
                "else:\n",
                "    print(\"OVERALL: ✗ DQN training has issues that need investigation\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
